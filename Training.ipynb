{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python385jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1",
      "display_name": "Python 3.8.5 64-bit"
    },
    "colab": {
      "name": "Training.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "accelerator": "GPU",
    "metadata": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8K0AzsLjMpZ"
      },
      "source": [
        "# NLP with LSTM Model\n",
        "|Title|Link|\n",
        "|-|-|\n",
        "|Zipf's Law|https://github.com/Ashvith/Zipf-s-Law/blob/master/zipfs_law.ipynb|\n",
        "|Next word prediction with NLP and deep learning|https://towardsdatascience.com/next-word-prediction-with-nlp-and-deep-learning-48b9fe0a17bf|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urHavV80jMpd"
      },
      "source": [
        "### Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krCjfBJvjMpe"
      },
      "source": [
        "# Inbuilt libraries, NLTK and pandas\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import pandas as pd     \n",
        "# from nltk.corpus import stopwords\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# Tensorflow and Keras libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow import keras\n",
        "import os\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5pOr8l5jMph"
      },
      "source": [
        "### Extracting all the words in the corpus\n",
        "  \n",
        ">If you're working with LSTMâ€™s or other models which capture the semantic meaning and the meaning of a word depends on the context of the previous text, then it becomes important not to remove stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUs4URI8jMpi"
      },
      "source": [
        "words_doc = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# nltk.corpus.gutenberg.fileids()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihyNxvzhjMpi"
      },
      "source": [
        "### Converting to lower case\n",
        ">Again, we will NOT filter the stopwords out for the same reason mentioned above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc38UXXbjMpj"
      },
      "source": [
        "data= [word.lower() for word in words_doc if word.isalpha()]\n",
        "data = ' '.join(data)\n",
        "# words_doc = [word for word in words_doc if word not in stop_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTqneY7FjMpk",
        "outputId": "67cd4b63-f4b9-4c9e-defb-a92b7a83ddac"
      },
      "source": [
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-r99dnxjU6h",
        "outputId": "c15dcd51-76cc-4241-d9ca-bae9a6fa7e66"
      },
      "source": [
        "# nltk.download('punkt')\n",
        "tokens = word_tokenize(data)\n",
        "train_len = 3+1\n",
        "text_sequences = []\n",
        "\n",
        "for i in range(train_len, len(tokens)):\n",
        "  seq = tokens[i-train_len:i]\n",
        "  text_sequences.append(seq)\n",
        "\n",
        "print(text_sequences[:20])\n",
        "sequences = {}\n",
        "count = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALGkYuFMmmmL",
        "outputId": "76a06a20-11a6-40be-9b84-df06436cf14a"
      },
      "source": [
        "for i in range(len(tokens)):\n",
        "  if tokens[i] not in sequences:\n",
        "    sequences[tokens[i]]=count\n",
        "    count +=1\n",
        "\n",
        "print(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5egjNlYIkjBE",
        "outputId": "b781907e-8b51-4e04-90f4-deb19b50d42f"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "\n",
        "# sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "# print(sequence_data)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "print(sequences[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQnV0lnDjMpl",
        "outputId": "d13f9dd0-b83a-4254-ae11-16ad6534b94a"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09HF4YnujMpm",
        "outputId": "a22b156c-4717-40d5-c3ce-9d4b4bc7fd94"
      },
      "source": [
        "n_sequences = np.array(sequences)\n",
        "\n",
        "for i in range(len(sequences)):\n",
        "    n_sequences[i] = sequences[i]\n",
        "print(n_sequences)\n",
        "\n",
        "# print(\"The length of sequences are: \", len(sequences))\n",
        "# sequences = np.array(sequences)\n",
        "# print(sequences)\n",
        "# # sequences[:10]\n",
        "\n",
        "# m_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
        "# for i in range(len(sequences)):\n",
        "#     m_sequences[i] = sequences[i]\n",
        "# print(m_sequences)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCo7xKKFkB1F",
        "outputId": "16944a89-7103-4d5d-f857-bda00c56fc55"
      },
      "source": [
        "x_train = n_sequences[:, :-1]\n",
        "y_train = n_sequences[:, -1]\n",
        "y_train = to_categorical(y_train, num_classes=vocab_size)\n",
        "seq_len = x_train.shape[1]\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o1XnCQjjMpn"
      },
      "source": [
        "# x_train = []\n",
        "# y_train = []\n",
        "\n",
        "# for i in sequences:\n",
        "#     x_train.append(i[0])\n",
        "#     y_train.append(i[1])\n",
        "\n",
        "# x_train = np.array(x_train)\n",
        "# y_train = np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXcMgeqyjMpo",
        "outputId": "545e0df1-91ee-48c1-f7dc-ce6adc34818f"
      },
      "source": [
        "print(\"The data is:\", x_train[:5])\n",
        "print(\"The responses are:\", y_train[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFp5l90KjMpo"
      },
      "source": [
        "# y_train = to_categorical(y_train, num_classes=vocab_size)\n",
        "# y_train[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itMwMedtjMpo"
      },
      "source": [
        "model = Sequential([\n",
        "    Embedding(vocab_size, seq_len, input_length=seq_len),\n",
        "    LSTM(512, return_sequences=True),\n",
        "    LSTM(512),\n",
        "    Dense(512, activation=\"relu\"),\n",
        "    Dense(vocab_size, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNHU1NnUjMpp",
        "outputId": "3d6b92e3-5130-4a35-afe3-c49576ba15cc"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "TEy_ihtTjMpp",
        "outputId": "59c00bcb-1ecd-4bbb-bcf4-a8664c8ac6b6"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAA9pBdIjMpq"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint =ModelCheckpoint(\"nextword3.h5\", monitor='loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "praGUf-ljMpq"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(\"nextword2.h5\")\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.0001), metrics=\"accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e77xvaJIjMpr",
        "outputId": "c6d08cd9-d2f4-488f-e9e0-3c4490f3c0b0"
      },
      "source": [
        " model.fit(x_train, y_train, epochs=100, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVl9N0MjrHfm"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9HVk_6jtJqG",
        "outputId": "629d8e78-9190-4aa3-d2a9-3362bdc4a193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow.keras.models import load_model\n",
        "# import numpy as np\n",
        "# import pickle\n",
        "\n",
        "# model = load_model(\"nextword2.h5\")\n",
        "# # tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
        "\n",
        "# input_text = input().strip().lower()\n",
        "# encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "# pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
        "# print(encoded_text, pad_encoded)\n",
        "# for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
        "#   pred_word = tokenizer.index_word[i]\n",
        "#   print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1_sE_au23md"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "model = load_model(\"nextword2.h5\")\n",
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
        "seq_len = pickle.load(open('seq_len1.pkl', 'rb'))\n",
        "\n",
        "input_text = input().strip().lower()\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
        "print(encoded_text, pad_encoded)\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
        "  pred_word = tokenizer.index_word[i]\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}